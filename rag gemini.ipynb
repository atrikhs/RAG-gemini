{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d49c22a-1ad0-4395-b93b-aa95660aa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Practical.pdf\")\n",
    "data = loader.load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9672410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 0}, page_content='Sandia Laboratories Project-AnomalyGPT\\nTeam SAM\\nSeungHoon Yoo\\nGeorgia Institute of Technology\\nsyoo97@gatech.edu@gatech.edu\\nAmit Trikha\\nGeorgia Institute of Technology\\namittrikha@gatech.edu\\nMukta Bisht\\nGeorgia Institute of Technology\\nmbisht6@gatech.edu\\nAbstract\\nFor this study, we utilized the AnomalyGPT model [1]\\nto specialize in detecting anomalies in our custom Art-\\nwork (paintings) dataset. Large vision-language models\\n(LVLM) excel in recognizing common objects due to their\\nextensive training data, they often struggle with domain-\\nspecific knowledge and fine-grained details within objects.\\nThis limitation impedes their effectiveness in domain spe-\\ncific tasks such as Artwork (or painting) Anomaly De-\\ntection. So,we investigated adapting the AnomalyGPT\\nmodel to our custom dataset to tackle the domain spe-\\ncific (i.e artwork/painting) problem. Our approach uti-\\nlizing the AnomalyGPT models,introduces a methodology\\nleveraging Large Vision-Language Models (LVLMs) to en-\\nhance anomaly detection capabilities in artwork settings.\\nBy training AnomalyGPT on our specific dataset,we aimed\\nto improve anomaly detection accuracy by integrating\\ndomain-specific knowledge directly into the model. This\\nrepresents a departure from traditional anomaly detection\\nmethods by offering a more automated and precise ap-\\nproach to distinguishing anomalies. Rather than relying\\nsolely on predefined thresholds,the model learns nuanced\\nfeatures and patterns inherent in anomalies,thereby facil-\\nitating more effective anomaly detection without extensive\\nmanual intervention. Through this study,we try to demon-\\nstrate the feasibility and effectiveness of adapting LVLMs\\nlike AnomalyGPT for specialized anomaly detection tasks.\\nAs part of our exploration into advanced anomaly detection\\ntechniques,we also investigate alternative large vision mod-\\nels beyond AnomalyGPT. Specifically,models such as CLIP\\n(Contrastive Language-Image Pretraining) and Anomaly-\\nCLIP were scrutinized for their suitability and efficacy for\\nthe artwork dataset. Each model underwent comprehensive\\ntraining on the dataset,followed by evaluation using estab-\\nlished performance metrics to gauge their detection capa-\\nbilities and overall performance.\\n1. Introduction\\nThe increasing prevalence of digital imagery in vari-\\nous sectors necessitates robust and reliable anomaly detec-\\ntion systems. Anomaly detection is a critical task in en-\\nsuring the integrity and quality of visual data,which can\\nhave significant implications across fields such as security,\\nhealthcare,and art curation. This project report delves into\\nthe development and evaluation of three innovative mod-\\nels—AnomalyGPT, AnomalyCLIP,and CLIP—designed\\nfor the classification and detection of anomalies in images.\\nEach model leverages different aspects of advanced ma-\\nchine learning techniques to achieve accurate and efficient\\nanomaly detection.The primary objective of this project was\\nto build and compare the performance of three distinct mod-\\nels for anomaly detection in images.Scope of Images were\\nrestricted to 2 classes, Mona Lisa and Girl with pearl ear-\\nring. Training images were around 50-100 and test images\\nhad 34 anomaly images and 16 non anomaly ones.\\n• CLIP Model: Utilizes the power of the CLIP (Con-\\ntrastive Language–Image Pre-training) model to clas-\\nsify images as either anomaly or non-anomaly.Trained\\nto classify images into anomaly and non-anomaly cat-\\negories based on their visual content\\n• AnomalyCLIP Model: An advanced iteration of the\\nCLIP model, specifically designed to not only clas-\\nsify images but also detect the presence of anoma-\\nlies within them.Enhanced to detect the presence of\\nanomalies within the images, providing a more nu-\\nanced understanding of the data\\n1'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 1}, page_content='• AnomalyGPT Model : A sophisticated model that\\nextends the capabilities of AnomalyCLIP by pro-\\nviding descriptive insights into the images and in-\\ndicating whether an anomaly is present. This\\nmodel features an interactive user interface similar\\nto ChatGPT,enhancing user interaction and accessibil-\\nity. Equipped with natural language processing ca-\\npabilities to describe the images and identify anoma-\\nlies,making it a versatile tool for detailed image analy-\\nsis.\\n2. Data\\n2.1. Base Dataset\\nAs discussed in the proposal and the midterm report,the\\nteam discovered Best Artworks of All Time dataset contain-\\ning over 3,000 images of artworks painted by 50 influen-\\ntial artists,including Claude Monet, Edgar Degas, and Andy\\nWarhol [2]. The team’s initial plan was to utilize the en-\\ntirety of over 3,000 images as ”good”, non-anomalous data\\nand generate anomalous paintings to test on by artificially\\nadding synthetic anomalies.\\n2.2. Adding Synthetic Anomalies\\nThe team developed Python codes using the OpenCV li-\\nbrary to artificially add anomalies to the original paintings\\ndataset. To ensure variety in resulting anomalies,the team\\ngenerated anomalies with:\\n• Different shapes including boxes,lines,letters,circles,\\nand dots.\\n• Random variations in size,location,and thickness\\n• Colors that match the overall color scheme and blend\\nmore with the artwork\\nFigure 1. Synthetic Anomaly Images\\n2.3. Pivoting to the Mona Lisa and the Girl with a Pearl\\nEarring\\nAs the team deepened the understanding of the anomaly\\ndetection models, the team realized that the initial plan\\nto use the entire paintings dataset was not realistic. This\\nis because anomaly detection models need to be trained\\non numerous ”good”,non-anomalous images of the same\\nor similar data. For example, AnomalyGPT model is\\ntrained on the MVTec-AD dataset which contains many\\nimages of cross sections of bottles,cable wires,etc that\\nlargely look similar in naked eyes [1]. As each artwork\\nis unique and distinct by nature, no similar artworks\\nexist and therefore no enough images are available for\\nadequate model training. Thus,the team decided to use\\nmultiple copies of the same painting for training, but\\ndoing so for 3,000 artworks would require excessive\\ncompute and storage capacity. As a result,the team decided\\nto use onlythe Mona Lisaand the Girl with a Pearl Earring.\\nIn addition to adding synthetic anomalies to the two art-\\nworks as described in 2.2,the team also included in the test\\ndataset fake replicas and artistic variations (artistic reinter-\\npretations) of the two artworks.\\nFigure 2. Tested Variations of the Mona Lisa and the Girl with a\\nPearl Earring\\n3. Infrastructure\\nAs a part of this project,we need to work with a large\\ndataset and train anomaly detection models using GPUs.\\nThis requires a high amount of RAM and CPU. For this rea-\\nson, we have leveraged Google Collab Pro. Later,we were\\nalso provided Linux Machine from Georgia tech to use.Our\\ninfrastructure requirements were below.\\n• Storage Capacity: 400 GB\\n• Storage capacity details:ImageBind - 5GBLlama - 20\\nGB (7B)\\n• Llama - 55 GB (13B)\\n• vicuna-7b-delta-v0 - 15 GB\\n• vicuna-13b-delta-v0 - 30 GB\\n• Combine Weights 7b - 40 GB (estimate)\\n• Combine Weights 13b - 90 GB (estimate)\\n• PandaGPT - 1GB\\n2'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 2}, page_content='• AnomalyGPT weights:\\n– Unsupervised on MVTec-AD - 225 MB\\n– Unsupervised on VisA - 225 MB\\n– Supervised on MVTec-AD, VisA, MVTec-\\nLOCO-AD and CrackForest-225 MBDataset\\n• Artworks - 2GB (compressed)\\n• MVTec-AD - 5GB (compressed)\\n• VisA - 2GB (compressed)\\n• PandaGPT - 15 GB (compressed)\\nSince this project requires specific infra requirements as\\nabove and GPU for training the models(AnomalyGPT and\\nAnomalyCLIP),it makes it challenging to get hold of GPU\\nwith considerable amount of compute units without them\\ngetting exhausted quickly because of the weights genera-\\ntion and training requiring high amounts of those.Google\\ncollab provides GPUs which were bought time to time.The\\nother alternative was the linux machine with GPUs provided\\nby GT,whose setup was easy but it only provided CLIs and\\nno UI like google drive/collab to interact properly. Weight\\nfiles being very huge (few GBs) would have to be scp to the\\nserver.Which will be time consuming because of the size.\\n4. Models Overview\\n4.1. AnomalyGPT\\nAnomalyGPT represents a revolutionary approach in\\nLarge Vision-Language Models (LVLMs) for its abil-\\nity to autonomously identify anomalies in industrial\\nimages,marking a significant advancement in Industrial\\nAnomaly Detection (IAD) [1]. The model achieves this\\nwithout the dependency on manually defined thresholds and\\noffering a efficient approach to anomaly detection in com-\\nplex industrial environments. Unlike existing IAD meth-\\nods that offer anomaly scores requiring manual threshold\\nconfiguration,and conventional LVLMs incapable of im-\\nage anomaly detection,AnomalyGPT excels in pinpointing\\nanomalies’ presence,location,and providing contextual im-\\nage details. Leveraging a pre-trained image encoder and\\nLVLM. The model aligns IAD images with textual de-\\nscriptions via simulated anomaly data. This approach in-\\ncludes a lightweight,visual-textual feature-matching image\\ndecoder for localization,and employs a prompt learner to\\nenhance LVLM semantic understanding via prompt embed-\\ndings,enabling anomaly detection even for novel items with\\nminimal normal sample data.\\nLarge Language Models (LLMs) are now being used for\\ntasks that involve both images and text. AnomalyGPT uses\\na Llama weights that refer to the parameters utilized in mod-\\nels within the Llama family provided by META. Weight\\nis a fundamental concept in neural networks,including the\\ntransformer-based language models like Llama [2]. Weights\\nare the adjustable parameters that the model learns during\\ntraining,enabling it to capture patterns in the data and per-\\nform well on natural language processing (NLP) tasks. The\\ntransformer architecture 4,which has become the prevalent\\ndesign for state-of-the-art language models,organizes these\\nweights into a specific structure named ”multi-head self-\\nattention”. MiniGPT4 [3] does this by connecting BLIP-2’s\\n[4] image part with the Vicuna [5] model through a special\\nlayer,refining its performance with lots of image and text\\ndata. Similarly,PandaGPT [6] links ImageBind [7] with the\\nVicuna [5] model to handle different kinds of information\\ntogether. While these models show promise in handling var-\\nious types of tasks,they often lack specific knowledge about\\nparticular fields because they are trained on broad datasets\\nthat cover many topics. To tackle this issue,AnomalyGPT\\npresents a new approach in this research. It uses simulated\\nabnormal data,incorporates advanced techniques for inter-\\npreting images, and employs prompt embeddings. Anoma-\\nlyGPT is designed to detect anomalies in data without need-\\ning predefined rules,and it can quickly learn from just a few\\nexamples in specific situations.\\nFigure 3 shows the architecture of AnomalyGPT. When\\nprovided a query image x ∈ RH×W×C, the image encoder\\nextracts its final features Fimg ∈ RC1. These features are\\nthen passed through a linear layer to create the image em-\\nbedding Eimg ∈ RCemb , which is then input into the Large\\nLanguage Model.\\nIn an unsupervised scenario,the image encoder’s inter-\\nmediate layers extract features from patches,which are then\\nused by the decoder along with text features to pinpoint\\nanomalies at the pixel level. The model operates without la-\\nbeled training data. The image encoder extracts patch-level\\nfeatures,which are combined with text features in the de-\\ncoder to generate detailed,pixel-level anomaly maps. This\\napproach relies solely on the intrinsic properties of the\\ndata to find anomalies. In a few-shot scenario, features\\nfrom normal samples’ patches are stored in memory banks.\\nTo find anomalies,the system calculates the distance be-\\ntween query patches and their closest matches in the mem-\\nory bank. These localization results are then converted\\ninto prompt embeddings using a prompt learner,which be-\\ncomes part of the input for the large language model (LLM).\\nThe LLM uses image input,prompt embeddings,and user-\\nprovided textual input to detect anomalies and identify their\\nlocations,thus generating responses for the user. The model\\nutilizes a small number of labeled examples to enhance its\\nperformance. Normal sample features are stored in mem-\\nory banks, which serve as a reference for the query image\\npatches. By comparing these patches to the stored features,\\nthe model can more accurately pinpoint anomalies. The dis-\\ntances between the query patches and their closest matches\\n3'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 3}, page_content='Figure 3. Architecture of AnomalyGPT.\\nin the memory bank are used to determine the anomaly\\nscores. These scores are then transformed into prompt em-\\nbeddings by the prompt learner.\\nThe combination of these embeddings with the ini-\\ntial image and any textual input from the user forms the\\ncomplete input to the LLM. It processes the comprehen-\\nsive input to detect and localize anomalies within the im-\\nage and then generates detailed,informative responses for\\nthe user,explaining the nature and location of the detected\\nanomalies. This architecture allows it to function effectively\\nin both unsupervised and few-shot learning scenarios.\\n4.2. CLIP\\nAdditionally,regular vision models like Contrastive\\nLanguage-Image Pretraining[8] was also tried to see how\\nthis model performed on our dataset. CLIP in general,tries\\nto maximize the “cosine similarity” between correct image-\\ncaption vector pairs,and minimize the similarity scores be-\\ntween all incorrect pairs. In inference,it calculates the sim-\\nilarity scores between the vector of a single image with\\na bunch of possible caption vectors,and picks the caption\\nwith the highest similarity. Note that CLIP is not a cap-\\ntion generation model,it can only tell you if some existing\\ntext caption fits well with an existing image or not. A con-\\ntrastive function that will modify the weights of the model\\nsuch that correct image-caption pairs get a high similarity\\nscore,and incorrect pairs get low similarity scores.[9] gives\\na good summary of it. We used CLIP with few different\\nCNN architectures like [10] which is a convolutional neural\\nnetwork (CNN) that’s a member of the ResNet (Residual\\nNetworks) family of models. It’s a supervised learning al-\\ngorithm that’s trained to predict labels or outputs based on\\ninput images. ResNet50 is often used for image classifi-\\ncation tasks.Loss function like [11] was used. Other algo-\\nrithms like [12],which detects anomalies using binary trees\\nwere used with CLIP. The algorithm has a linear time com-\\nplexity and a low memory requirement,which works well\\nwith high-volume data. It uses decision trees to efficiently\\nisolate anomalies by randomly selecting features and split-\\nting data based on threshold values. This approach is ef-\\nfective in quickly identifying outliers,making it well-suited\\nfor large datasets where anomalies are rare and distinct.[13]\\nwas another algorithm which was used in conjunction with\\nCLIP,it is an unsupervised anomaly detection method which\\ncomputes the local density deviation of a given data point\\nwith respect to its neighbors. It considers as outliers the\\nsamples that have a substantially lower density than their\\nneighbors.[14] tells what happens in the background. We\\nused supervised learning approach with all algorithms pro-\\nviding test data with the actual labels.\\n4.3. AnomalyCLIP\\nAnomalyCLIP is an adaption of the CLIP model specifi-\\ncally designed for anomaly detection in images. It is a zero-\\nshot anomaly detection (ZSAD) model that tends to demon-\\nstrate an enhanced zero-shot recognition ability compared\\nto CLIP because it is trained to detect and capture generic\\nnormality and abnormality rather than focusing on the ob-\\nject semantics such as the foreground objects in the image.\\nFor example,in the case of the Mona Lisa painting,the reg-\\nular CLIP model would focus on the women in portrait as\\nthe most foreground element. However,the AnomalyCLIP\\nwould largely ignore the women and look for general pat-\\ntern that either fits as normal or deviates as abnormal.[15]\\nThis ability to identify normality and abnormality is a\\nmajor characteristic of AnomalyCLIP, a ZSAD model. The\\nAnomalyCLIP model is trained using auxiliary data that do\\nnot necessarily resemble or are directly related to the target\\ndataset its model is tested and used on. This is an important\\nstrength that makes this model more versatile and applica-\\nble to various unseen cases. In the paper that first intro-\\nduced AnomalyCLIP, the regular CLIP model was adapted\\nby fine-tuning with the MVTec AD dataset and its ZSAD\\nperformance was tested on non-MVTec AD dataset such as\\nVisA and MPDD. Similarly, in this project, the team lever-\\naged the existing pre-trained AnomalyCLIP model to eval-\\nuate its ZSAD performance against the unseen paintings\\ndata—the Mona Lisa and the Girl with a Pearl Earring.[15]\\n5. Model Setup and Training\\nWe trained 3 different models, AnomalyGPT, CLIP and\\nAnomalyCLIP for our dataset. This was done to see how\\ndifferent version of anomaly detection models behave and\\nrespond to our dataset. All three models were trained\\non the same dataset comprising original same images of\\ntwo famous paintings, ¨Mona Lisa ¨and ¨Girl with Pearl Ear-\\nring¨. However, Anomaly GPT and Anomaly CLIP re-\\nquired additional GPU infrastructure for training due to\\ntheir complexity, this setup was essential to handle the com-\\nputational complexity and large datasets required for these\\nmodels,while the standard CLIP model could be run lo-\\ncally without such requirements. ¡FOR EACH MODEL\\nADD FEW SENTENCES WHICH W AS DONE SPECIFI-\\n4'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 4}, page_content='CALLY FOR THAT MODEL DURING TRAINING AND\\nSETUP¿\\n5.1. AnomalyGPT\\nOur implementation for AnomalyGPT begins with repli-\\ncating the methodology outlined in the original Anoma-\\nlyGPT research paper, which focuses on utilizing indus-\\ntrial images for effective anomaly detection and then sub-\\nsequently training the model on a new dataset. The initial\\nstep involved securing access to the model weights and in-\\ntegrating them into both the training and inference phases\\nof our project. This process ensures that we leverage the\\nestablished capabilities of AnomalyGPT in our specific do-\\nmain of interest that is industrial dataset. Later, we sourced\\na new Artworks dataset, expanding beyond the industrial\\ncontext studied in the original paper. Artwork dataset is then\\naugmented with anomalies, introduced manually or through\\nautomated procedures (python notebooks), to simulate real-\\nworld scenarios where anomalies may manifest in the im-\\nagery. The goal was to train the AnomalyGPT model using\\nthis new dataset, enabling it to perform anomaly detection\\ntasks within the domain of Artworks.\\nWe ensured the proper environment configuration by in-\\nstalling the required packages listed in the repository’s re-\\nquirements file. We downloaded essential pre-trained im-\\nage encoders and a Large Language Model (LLM) to align\\nimages with their textual descriptions, creating simulated\\nanomaly data. These pre-trained model weights and reposi-\\ntories were obtained, and the scripts were run initially using\\nVS Code to prepare the weights in the necessary Hugging\\nFace and combined formats. The model employs a special-\\nized image decoder that matches visual and textual features\\nto achieve localization results in the images. Additionally,\\nit incorporates a prompt learner that enriches the LLM’s un-\\nderstanding by adding detailed meanings and adjusting the\\nLVLM with prompt embeddings. This initial setup ensures\\nthat we capture the capabilities of AnomalyGPT within our\\nspecific industrial dataset domain. The following steps were\\nundertaken:\\n• Setting up the Cloud environments: To successfully\\nbegin running the demo, the first step involved set-\\nting up the essential cloud environments. Anoma-\\nlyGPT relies on multiple model weights and requires\\nsubstantial storage capacity. Additionally, the load-\\ning and training of these weights necessitate power-\\nful GPUs. Therefore,the entire codebase and weights\\nhad to be transferred to a cloud storage solution like\\nGoogle Drive. Google Colab was selected for utiliz-\\ning GPUs for efficient computation and also for its ca-\\npability to support complex model training and infer-\\nence tasks. This provided the necessary computational\\npower without requiring local hardware upgrades, en-\\nabling seamless execution of the demo. By leveraging\\nFigure 4. The Transformer Model Architecture\\nGoogle Colab, the setup process streamlined access to\\nGPU resources, crucial for handling the computational\\ndemands of AnomalyGPT’s training and deployment\\nphases. It provided a robust foundation for handling\\nthe intensive computational requirements of Anoma-\\nlyGPT, ensuring smooth operation and efficient utiliza-\\ntion of resources throughout the project lifecycle.\\n• Cloning the Repository: In order to replicate Anoma-\\nlyGPT, we started by cloning the Anomaly GitHub\\nrepository [16]. The repo was loaded to cloud storage\\nand executed from Colab. Additionally, we installed\\nall required packages for the demo as outlined in the\\nrepository’s requirements.txt file. This GitHub reposi-\\ntory encompassed the complete codebase and directory\\nstructure essential for running the demo.\\n• Downloading and preparing checkpoint weights:\\nPrepare ImageBind Checkpoint : We downloaded\\nthe pre-trained ImageBind model [7] and stored it in\\nthe specified directory for the Demo. ImageBind is\\na method designed to create a unified representation\\nacross six types of data: images,text,audio,depth, ther-\\nmal,and IMU data. This method shows that achieving\\nthis unified representation doesn’t require training on\\nevery possible combination of paired data. Training\\n5'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 5}, page_content='solely with image-paired data suffices to link all these\\ndiverse data types together.\\nPrepare Vicuna Checkpoint : In AnomalyGPT,the\\nlanguage decoder depends on Vicuna version 0, which\\nneeds to be integrated with the LLAMA weights from\\nMeta. Llama weights refer to the parameters uti-\\nlized in the models within the Llama family based\\non Transformer Architecture 4. Due to the distri-\\nbution license of LLaMA,we had to manually re-\\nstore the Vicuna weights after obtaining approval from\\nMeta. Initially,we restored the 7B version of Vicuna\\nv0. Once we acquired the 7B LLaMA weights,we\\nfollowed instructions to convert them into the Hug-\\nging Face format. Later,we downloaded the delta\\nweights of Vicuna provided by the original authors,\\nspecifically using the 7B Vicuna model’s delta weights\\n(lmsys/vicuna-7b-delta-v0). The final step\\ninvolved combining these two sets of weights using the\\nFastchat tool provided by the Vicuna team. We also at-\\ntempted to use Vicuna-13B (version 0) weights, but en-\\ncountered irreconcilable errors when trying to combine\\nthem with the delta weights. Therefore,we decided to\\nproceed with Vicuna-7B instead.\\nPrepare Delta Weights for AnomalyGPT :\\nWe obtained pre-trained parameters from\\nPandaGPT to initialize our model. Using Vicuna-\\n7B and the pandagpt 7b max len 1024\\n(pytorch model.pt),we placed these weights\\nin the specified directory.\\nAnomalyGPT weights: Finally,below weights for dif-\\nferent dataset were were downloaded and added in the\\n./code/ckpt/ directory in the Google Drive\\nSetup and Datasets Weights Address\\nUnsupervised MVTec-AD train mvtec\\nUnsupervised VisA train visa\\nSupervised MVTec-AD, VisA,\\nMVTec-LOCO-AD,CrackForest\\ntrain supervised\\nTable 1. Weights addresses in AnomalyGPT\\n• Running AnomalyGPT DEMO : After completing\\nthe previous tasks,we proceeded to run the demo lo-\\ncally by navigating to the code directory and execut-\\ning the web demo.py command. Figures 5 showcase\\nimages and results from the AnomalyGPT Demo user\\ninterface. From the results,it is evident that the applica-\\ntion correctly identifies anomalous and non-anomalous\\nimages.\\n• Training on Artwork Dataset : After successfully\\nrunning the demo, the next step was to train the model\\non a new dataset. For this, we selected the Artwork\\ndataset, focusing on two iconic images: ”Mona Lisa”\\nand ”Girl with a Pearl Earring.” Our goal was to ex-\\nplore how the model performs with art-related data and\\nto investigate its ability to recognize and adapt to vari-\\nous transformations applied to these images. We began\\nby developing a script to generate different transfor-\\nmations of the selected images. These transformations\\nincluded blurring, inverting, tilting,saturation changes,\\nand other alterations. The purpose was to create a di-\\nverse training set that would help the model learn to\\nidentify and handle various modifications and distor-\\ntions of the original artworks.\\nOnce the transformed images were prepared, we\\nstrated working on developing the training scripts.\\nThese scripts were specifically developed for the\\nArtwork dataset and were designed to optimize\\nthe training process on Google Colab’s powerful\\nGPUs,specifically the A100 or L4, depending on avail-\\nability. The training was conducted over three different\\ndurations: 15,20,and 25 epochs. This range allowed\\nus to observe how the model’s performance evolved\\nwith varying amounts of training. For training,we set a\\nlearning rate of 1e-3 and experimented with two batch\\nsizes: 8 and 16. These parameters were chosen based\\non standard practices and initial experimentation to en-\\nsure a balance between training speed and model ac-\\ncuracy. During this training process, only the decoder\\nand prompt learner underwent parameter updates. All\\nother parameters were kept frozen to focus the learning\\nprocess on these specific components of the model.\\n• Training Configurations The table 2 shows the train-\\ning hyperparameters used in our experiments. The hy-\\nperparameters are selected based on the constraints of\\nour computational resources, i.e. A100 or L4 GPUs\\nGoogle Colab.\\nModel Epochs Batch Learning Rate MaxLength\\nVicuna-7B 15 8 1e-3 1024\\nVicuna-7B 15 16 1e-3 1024\\nVicuna-7B 20 16 1e-3 1024\\nTable 2. Training hyperparameters used in our experiments\\nThroughout the training process,we closely monitored\\nthe model’s performance,particularly its ability to rec-\\nognize and accurately classify the transformed images.\\nBy applying various transformations to the images,we\\naimed to simulate real-world scenarios where artworks\\nmight be subjected to different viewing conditions or\\nalterations. This approach helped in assessing the ro-\\nbustness and adaptability of the model. The Demo ap-\\nplication was then updated to use the newly created\\nArtwork training weights. Figures 6, 7 display the out-\\n6'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 6}, page_content='comes of various test cases, showcasing the applica-\\ntion’s performance with the new weights.\\nFigure 5. AnomalyGPT Demo UI - Image of Bottle, Transistor\\nwith Anomaly and Carpet without Anomaly\\n5.2. CLIP\\nFor the training purposes, we used 50-100 non anomaly\\nimages for model to get trained on.Since our focus has been\\ndrilled down to 2 classes mainly ie: Mona Lisa and Girl\\nwith pearl earring, we only had these 2 classes for model to\\nlearn from. One project has all the python files that would\\nneed to be run individually to see the output. Each of them\\ncreates some output folder to store the classified images as\\nanomaly vs. non anomaly.ReadME.md has all details on\\nwhat each file does,input,output and execution instructions\\nin it.We referred [17] and [18] to start with.All the training\\nis mainly done with more than one epoch to let model learn\\nbetter from non anomaly images for both classes.With algo-\\nrithms like LOF,Isolation Forest and resnet50 architecture\\nusing cross entropy function,main approach was to adjust\\nthe parameters like threshold,contamination etc which helps\\nmodel to determine how to differentiate in order to segre-\\ngate between anomaly and non anomaly after it calculates\\ncosine similarity.Hence,as we expect some manual tweaks\\nare required for these parameters to adjust them for model to\\nlearn better.Out of all algorithms,resnet50 was slowest and\\nwas taking around 5-10 min to finish the run.Others took 2\\nmin on an average to finish.\\n5.3. AnomalyCLIP\\nAs described previously,AnomalyCLIP is a zero-shot\\nanomaly detection model whose strength lies on how the\\nmodel can successfully generalize to target dataset that is\\nnot part of the data used for training the model. Thus,the\\nteam decided to use the pre-trained AnomalyCLIP weights\\nwithout re-training it on the paintings dataset and evaluate\\nif it shows expected ZSAD performance.\\nHowever,it is still worthwhile to briefly discuss how the\\nscientists behind the AnomalyCLIP model implemented the\\nmodel. The AnomalyCLIP team based its model on the\\npublicly available regular CLIP model, specifically VIT-\\nL/14@336px. The model parameters of the CLIP model are\\nkept unchanged during the training of the AnomalyCLIP\\nmodel. The length of learnable text prompts is set to 12.\\nThe learnable token embeddings are attached to the first 9\\nlayers of the text encoder for refining the textual space, and\\ntheir length in each layer is set to 4. To further fine-tune the\\nmodel, the scientists used the industrial MVTec AD dataset.\\nIt is this AnomalyCLIP model,fine-tuned with the MVTec\\nAD data, that the team used to test its ZSAD performance\\non the Mona Lisa and the Girl with a Pearl Earring.[15]\\nIn order to use the pre-trained Anomaly-\\nCLIP model,the team followed the instructions\\nin the model developers’ GitHub repository:\\nhttps://github.com/zqhang/AnomalyCLIP. The reposi-\\ntory includes all scripts needed to run the model on test\\nimages of various industrial anomaly datasets. The team\\nmodified the script mvtec.py to ensure it is compatible with\\nthe structure of the folder containing normal and abnormal\\nimages of the Mona Lisa and the Girl with a Pearl Ear-\\nring. The modified python scripts are titled monalisa.py\\nand pearl earring.py, respectively. Similarly,the original\\ntest.sh file was modified to create monalisa test.sh and\\npearl earring test.sh shell files.\\n6. Model Testing\\nAll the three models were tested using same test images\\nwhich comprised 34 anomaly and 16 non anomaly images.\\nBelow is the brief summary of testing for each model.\\n7'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 7}, page_content='6.1. AnomalyGPT\\nTo test our system,we implemented an anomaly detec-\\ntion mechanism for images of artworks using a pre-trained\\nmodel,OpenLLAMAPEFTModel. This model leverages\\nfew-shot learning to enhance detection capabilities with\\nlimited examples. We initialized the OpenLLAMAPEFT-\\nModel with specified parameters and loaded pre-trained\\ncheckpoints from designated paths. The model was then\\nset to evaluation mode and moved to the GPU.\\nThe prediction function generates responses and\\nanomaly maps for input images using the model. It con-\\nstructs a prompt based on the input and the history of pre-\\nvious interactions. The predicted anomaly maps are com-\\npared with ground truth masks to calculate pixel-level and\\nimage-level anomaly detection metrics.\\n• Precision Metrics:\\nPixel-level AUROC (Area Under the Receiver\\nOperating Characteristic Curve) : This measures\\nthe model’s ability to distinguish between normal and\\nanomalous pixels across the entire image.\\nAccuracy: calculates the percentage of correct\\npredictions out of the total predictions made by the\\nmodel for each class.\\nFor each class, we captured the number of correct and\\nincorrect predictions, pixel-level AUROC and finally\\ncomputes the average pixel-level AUROC and accu-\\nracy across classes.\\nPaintings Pixel-Level AUROC Accuracy\\nMona Lisa 62.3 61.2\\nPearl Earring 59.7 58.2\\nTable 3. Artwork Performance For AnomalyGPT\\nFigure 6. Artwork Trained Anomaly Detection: Girl with a Pearl\\nEarring\\nFigure 7. Artwork Trained Anomaly Detection - Mona Lisa\\n6.2. CLIP\\nTest dataset consisted of 34 anomaly images and 16 non\\nanomaly images for both classes. The structure for test\\ndataset was set up differently as compared to train, where it\\nhad only folders for each class with clean images. Here,we\\nhad 2 folders for each label,anomaly and non anomaly with\\neach having both classes inside them. This is essential for\\nmodel to the real labels in order to calculate the accuracy\\nagainst test data.The test data consists of diverse anomalies\\nfrom both classes. Once the model runs on few epochs, it\\nmatches checks the threshold value vs score it got,vs simi-\\nlarity score etc. Since each algorithm we used with CLIP,\\nlogically works different,the classification depends on their\\nown logic. Below is the brief summary of each.\\n• CLIP(CrossEntropy loss function) : This loss func-\\ntion commonly used for classification tasks, measures\\nthe difference between two probability distributions:\\nthe predicted probability distribution (output by the\\nmodel) and the actual distribution (true labels). It\\nquantifies how much the predicted probabilities di-\\nverge from the true labels. It calculates the loss for\\na single instance as -log(P(y)),where P(y) is the pre-\\ndicted probability for the true class y. In training neu-\\nral networks,this loss is minimized using optimiza-\\n8'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 8}, page_content='tion techniques like gradient descent. This minimiza-\\ntion process adjusts the model weights to improve pre-\\ndiction accuracy. In our model code,anomaly detec-\\ntion is performed using the CLIP model and a cus-\\ntom classifier. Once the CLIP model is loaded,and\\nthe device (CPU or GPU) is set up,features ex-\\ntracted,transformations are done to preprocess the im-\\nages (resize and convert to tensors). Image datasets\\nare created from the specified directories,and dataload-\\ners are set up to iterate through the data in batches. A\\ncustom classifier is defined,consisting of two fully con-\\nnected layers with ReLU activation. Optimizer used is\\nAdam for optimizing the classifier’s weights. A cus-\\ntom function generates synthetic anomalies by apply-\\ning random transformations (color jitter and Gaussian\\nblur) to the training images. The classifier is trained on\\nthe combined dataset of original non-anomaly images\\nand synthetic anomalies: Overall,It extracts features\\nusing the CLIP model,Labels are created (0 for non-\\nanomaly and 1 for synthetic anomaly),loss is calcu-\\nlated using CrossEntropyLoss. The model is optimized\\nusing backpropagation. Predictions are made using the\\nclassifier.Accuracy is calculated by comparing predic-\\ntions with true labels. Images are saved into respective\\noutput directories based on the predictions (anomaly or\\nnon-anomaly). The model was able to correctly clas-\\nsify 25 non anomaly images as non anomaly but 8 were\\nmisclassified as non anomaly. All 16 non anomaly im-\\nages were correctly classified as non anomaly. Overall\\nit was able to achieve,83.67% accuracy.\\n• CLIP(resnet50): ResNet-50 is a deep convolutional\\nneural network that is 50 layers deep. It is part of the\\nResNet (Residual Networks) family,which introduced\\nthe concept of residual learning to tackle the problem\\nof vanishing gradients in very deep networks. Our\\ncode uses a combination of a pre-trained ResNet-50\\nmodel(to extract features) and the CLIP model (to get\\nthe image-text embedding) to classify test images as\\neither ”anomaly” or ”non-anomaly.” The classification\\nis based on the cosine similarity between the test image\\nembeddings and the embeddings of the original paint-\\nings (”Mona Lisa” and ”Girl with a Pearl Earring”).\\nThe classification results are saved in separate folders\\nfor anomalies and non-anomalies. The model was able\\nto correctly classify all non anomaly images as non\\nanomaly and all anomaly images were correctly clas-\\nsified as anomaly. Overall it was able to achieve,100%\\naccuracy.This model used the check where similarity\\nmeasure has to be greater than equal to 1 to be classi-\\nfied as non anomaly.\\n• CLIP(iso forest) : IsoForest is an anomaly detec-\\ntion algorithm that is particularly well-suited for high-\\ndimensional datasets. It works by isolating observa-\\ntions in a dataset. The basic premise is that anomalies\\nare few and different,thus they are more susceptible\\nto isolation. The algorithm creates multiple decision\\ntrees, known as isolation trees or iTrees,by randomly\\nselecting a feature and then randomly selecting a split\\nvalue between the minimum and maximum values of\\nthe selected feature. This process of randomly select-\\ning features and split values continues recursively until\\neach data point is isolated or the tree reaches a pre-\\ndefined height. The number of splits required to iso-\\nlate a data point is termed as the path length. Anoma-\\nlies,being few and different,are more likely to be iso-\\nlated sooner (i.e, in fewer splits) compared to nor-\\nmal data points. For instance,if a data point is iso-\\nlated by splitting on one or two features,it is likely an\\nanomaly. The anomaly score is based on the average\\npath length from all the trees. The shorter the aver-\\nage path length,the more anomalous the point is con-\\nsidered. The anomaly score for a point is computed\\nas:(x,n)=2ˆ(E(h(x)c(n)), where E(h(x)) is the average\\npath length for point x and ( ¸n).( ¸n) is a normalization\\nfactor representing the average path length of an un-\\nsuccessful search in a Binary Search Tree. A threshold\\nis set for the anomaly score. Points with scores above\\nthe threshold are classified as anomalies. Raw pixel\\nvalues of images are high-dimensional and may not be\\ndirectly useful for anomaly detection. Instead, mean-\\ningful features are extracted using a pre-trained model\\nlike CLIP. The extracted features represent the image\\nin a lower-dimensional space that captures the impor-\\ntant characteristics of the image. These features are\\nused to train the Isolation Forest model.In our code,it\\nchecks the anomaly score to the threshold , if its less\\nthan threshold then its anomaly else not. With just 10th\\npercentile of np˙percentile(anomaly scores 10),it is not\\nable to pick up all anomalies we have in the test im-\\nages. When set to 60 it performed much better. Also\\ncontamination part was updated from 0.2 to 0.4. This\\nis all done to pick up all anomalies that can reside in\\nany part of test image. The model was able to classify\\nall non anomaly images correctly but 4 images were\\nmisclassified as non anomaly when they were actually\\nanomalies.Overall it was able to achieve,91.11% accu-\\nracy.\\n• CLIP(LOF): The Local Outlier Factor (LOF) algo-\\nrithm is an unsupervised anomaly detection method.\\nIt identifies anomalies based on the local density devi-\\nation of a data point compared to its neighbors. Local\\ndensity calculation(LDC)For each data point,the algo-\\nrithm calculates the local density by considering the\\ndistance to its nearest neighbors.Local Reachability\\nDensity (LRD) is the reachability distance of a point A\\n9'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 9}, page_content='with respect to point B is the maximum of the distance\\nbetween A and B and the distance to the k-th nearest\\nneighbor of B. LRD of a point is the inverse of the\\naverage reachability distance from its neighbors. The\\ndensity is estimated based on the inverse of the average\\ndistance to the k-nearest neighbors. The LOF score for\\na point is the average ratio of the LRD of its neighbors\\nto its own LRD. A higher LOF score indicates a greater\\nlikelihood of being an outlier,as it suggests that the\\npoint has a significantly lower density compared to its\\nneighbors. A threshold is set for the LOF score to clas-\\nsify points as anomalies. Points with scores above this\\nthreshold are considered anomalies. In our code,LOF\\nis used to detect anomalies in image features extracted\\nby the CLIP model. The CLIP model provides a mean-\\ningful representation of the images,and LOF identi-\\nfies images that deviate significantly from the normal\\npatterns in these representations. The training dataset\\nis augmented with random horizontal flips,rotations,\\nand color jitter,followed by the preprocessing steps re-\\nquired by the CLIP model.The test dataset is only pre-\\nprocessed without augmentation. Data loaders are cre-\\nated for the training and test datasets with batch sizes\\nof 32. The feature extraction function iterates over\\nthe data loader, passes the images through the CLIP\\nmodel to extract features,and collects these features\\nand their corresponding labels. A Local Outlier Factor\\n(LOF) model is initialized and trained on the extracted\\ntraining features.n neighbors is set to 20, novelty to\\nTrue (allowing the model to be used for prediction),and\\ncontamination to 0.45 (indicating the expected propor-\\ntion of anomalies in the dataset). True labels for the\\ntest dataset are created based on the presence of the\\nword ’anomaly’ in the file paths. The LOF model pre-\\ndicts anomalies in the test features.The predictions are\\nconverted to binary labels (1 for anomaly, 0 for non-\\nanomaly). Accuracy is calculated and results are are\\ncopied into appropriate directories (anomaly or non-\\nanomaly) based on the predictions. The model was\\nable to classify all non anomaly images correctly but\\nonly 1 image was misclassified as non anomaly when\\nthey were actually anomalies. Overall it was able to\\nachieve,97.62% accuracy.\\n6.3. AnomalyCLIP\\nThe team tested the AnomalyCLIP model on the Mona\\nLisa and the Girl with a Pearl Earring. The scripts included\\nin the original AnomalyCLIP repository return the Area Un-\\nder the Receiver Operating Characteristic Curve (AUROC)\\nas the performance metrics. We report these results here\\nalong with an additional metric, the model accuracy.\\nThe metrics above show combined results of all types\\nof various anomalies ranging from synthetic dots, boxes,\\nPaintings AUROC Accuracy\\nMona Lisa 70.5 75.6\\nPearl Earring 76.4 66.7\\nTable 4. ZSAD Performance of AnomalyCLIP\\nlines, and letters to reinterpretations of the artworks. Rein-\\nterpretations,shown in Figure 2, are much more subtle and\\nnuanced than synthetic anomalies in Figure 1. And this\\nseems to have affected the performances of the Anomaly-\\nCLIP model in capturing anomalies. While the model was\\nable to detect many of the synthetic anomalies, it was highly\\nlikely to fail to detect subtle variations in the artworks, of-\\nten identifying normal regions of the image as abnormal.\\nIn the case of the Girl with a Pearl Earring, the model de-\\ntected the pearl earring itself as anomalous,similar to how it\\ndetected circles as anomalous. Also,even amongst the syn-\\nthetic anomalies, the ones that have colors and sizes that are\\nless visible and obvious against background and surround-\\ning regions (e.g. brown boxes located in the Mona Lisa’s\\ndark silk dress) were less likely to be detected by the model.\\nFigure 8. Results of Anomaly Detection by AnomalyCLIP\\nAlso,it is important to note that AnomalyCLIP made\\nfalse detections, even in images for which the model cor-\\nrectly detected anomalies. Although these instances were\\nconsidered detections in the accuracy measure, addressing\\nthe false positives would be a challenging obstacle in using\\nthe AnomalyCLIP model for more sophisticated scenarios.\\n7. Challenges\\nOur dataset comprises of images that have paintings\\nfrom 2 classes, while trained images did have same orig-\\ninal image to be trained on for model to learn the correct\\nnon anomaly image,the test dataset comprised of various\\nvariations of anomalies which were not limited to just one\\ntype , rather random and different. Since the anomaly could\\nbe present anywhere in the image and could be anything,it\\nwas challenging overall for the models to get trained which\\ncould result in a good accuracy as well as their detection.\\nWhile some models like CLIP were able to classify them,\\nbut that would have risk of getting overfit and also require\\n10'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 10}, page_content='manual intervention for tweaking hyperparameters used in\\norder to classify images better. Below is the brief summary\\nof challenges faced for each.\\n7.1. AnomalyGPT\\n• Infrastructure set-up : We subscribed for Google\\nColab Pro as it provides GPUs needed for training\\nAnomalyGPT and running the web demo. How-\\never,this gives only a limited number of compute run-\\ntime and can exhaust easily. Also,Google Drive auto-\\nmatically stores the weights generated and other files\\ncreated during the set-up process but can stop if there\\nis not enough RAM left for storage. Once mounted to\\nGoogle Colab,files in Google Drive only stays for 12\\nhours,which means some tasks need to be repeated to\\namend for lost jobs.\\n• Large Weight Files : The weights for pretrained\\ncheckpoints like ImageBind Checkpoint,Vicuna,and\\ndelta are large ( 4GB files). During the generation\\nof these files,we had technical errors related to ver-\\nsion requirements and dependencies. Some had to\\nbe upgraded/changed to a new version. Some errors\\npertained to OS library,zip directory,and pickle error.\\nAlso,while running the AnomalyGPT demo,we had to\\nre-upload the weights for pandasGPT.\\n• 13B Weights : We attempted to utilize the 13B\\nweights instead of the 7B but encountered multiple\\nerrors when combining them with the Vicuna delta\\nweights. Also,13B weights required a considerably\\nlarger amount of space and computing power com-\\npared to 7B,adding to the challenges encountered dur-\\ning testing.\\n• Vast Training Times : Training the AnomalyGPT\\nmodel requires vast training times due to several fac-\\ntors. The model employs a complex architecture\\nthat integrates pre-trained components like OpenL-\\nLAMAPEFT and image processing layers,that needs\\nheavy computation. The training process involves\\nlarge datasets with high-resolution images and that fur-\\nther increasing computational demands. Fine-tuning\\nthe model on specific anomaly detection tasks also re-\\nquires significant iterations to achieve optimal perfor-\\nmance.\\n7.2. CLIP\\nThere were no challenges in order to prep or set up regu-\\nlar vision model as they can use CPU and also,do not require\\nany different infrastructure set up as our previous men-\\ntioned models do. It can be run locally on a VScode or py-\\ncharm with required requirements.txt file. Anomalies con-\\nstituted of marks like circle,dot,line,different color,different\\nface,fuzziness etc. Any minor deviation from original paint-\\ning should be detected by model. Since most of them\\nrequired manual tweaks for model to learn more appro-\\npriately given the anomalies could have resided anywhere\\nin the whole painting. We had to try different thresh-\\nold,contamination percentage etc. to achieve a better accu-\\nracy. Model was not introduced to any of the anomaly im-\\nages rather only clean images. Anomalies were diverse and\\ngiven the nature of our dataset,it was not straightforward to\\ntrain the model to detect any anomaly which it was not in-\\ntroduced before. With all the regular vision model CLIP\\nused along with algorithms mentioned highly depended on\\nsetting threshold,contamination parameter etc. There is no\\ndynamic thresholding,rather static. Which means the model\\nhas to be set in such a way that it is able to gauge the anoma-\\nlies anywhere in the image. This requires manual interven-\\ntion. Which can also challenge on the generalization part\\nof model. Once tested on given test images for anomalies,it\\nmay or may not perform that well for new anomalies if the\\nnew image is drastically different from what it was trained\\nand tested on earlier.Which also means, shifting the thresh-\\nold and other parameters. This kind of model would easily\\nget overfit and may not generalize well for diverse images.\\n7.3. AnomalyCLIP\\nInfrastructure set-up : Similar to AnomalyGPT,GPU is\\nneeded to run the AnomalyCLIP model. Thus,we sub-\\nscribed for Google Golab Pro. However,this gives only a\\nlimited number of compute runtime and can exhaust easily.\\nThis would be a challenge if we were to continue to amend\\nand improve the model for repeat uses.\\nPerformance on subtle anomalies : As noted in 6.3,\\nAnomalyCLIP did not show a strong performance in detect-\\ning subtle and nuanced anomalies. Also,the model made\\nfalse detections even in images for which it correctly de-\\ntected anomalies. This would be a major challenge to over-\\ncome in order for this model to be used widely.\\nSourcing data for fine-tuning : Although AnomalyCLIP\\nmodel’s main strength is its zero-shot anomaly detection\\nability that generalizes to unseen dataset, the model can\\nbenefit from fine-tuning with images from the same or re-\\nlated domains However,obtaining such relevant images is\\na challenge at times due to data privacy and security con-\\ncerns. The scientists behind the AnomalyCLIP model also\\nfaced this challenge when applying the model to medical\\nimages. The team explored various options and eventually\\naddressed the issue by combining multiple medical datasets\\nfrom different sources.\\n8. Comparison between models\\nThe table 5 offers a detailed comparison of accu-\\nracy scores for different models tested on a dataset,\\n11'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 11}, page_content='specifically highlighting the performance of the CLIP,\\nAnomalyCLIP,and AnomalyGPT models with various\\nalgorithms. The CLIP model is evaluated using\\nResNet50,CrossEntropy,Isolation Forest,and Local Outlier\\nFactor (LOF) algorithms, achieving the highest accuracy\\nscore of 100 with ResNet50. AnomalyCLIP model scores\\nan accuracy of 71.11, suggesting it is less effective than\\nthe best-performing CLIP configurations. AnomalyGPT\\nrecords an even lower accuracy score of 59.7 indicating\\na notable performance gap between it and the other mod-\\nels. The substantial differences in accuracy scores between\\nAnomalyGPT and the top-performing CLIP model under-\\nscore the varying effectiveness of different models and algo-\\nrithms on the test dataset. The test results highlight the com-\\nplexity of the models and the challenges in training them\\non datasets. Capturing intricate details proves to be particu-\\nlarly difficult. This complexity arises from the sophisticated\\nnature of the models, requiring extensive computational re-\\nsources and fine-tuning to achieve accurate results. The\\ntraining process involves navigating various parameters and\\nconfigurations to optimize performance,making it a daunt-\\ning task. Additionally,the models must be capable of rec-\\nognizing subtle patterns and nuances within the data,which\\nadds another layer of difficulty.\\nModel Algorithm Accuracy score\\nCLIP resnet50 100\\ncrossentropy 83.67\\niso forest 91.11\\nLOF 97.62\\nAnomalyCLIP - 71.11\\nAnomalyGPT - 59.7\\nTable 5. Accuracy Score Comparison for Different Models on Test\\nDataset\\n9. Learning and Future enhancements\\n9.1. Key Learnings\\n• Model Performance and Infrastructure: One of the\\nmost significant learnings from this project was the\\ncritical role of infrastructure in model training and de-\\nployment. Given the computational demands of mod-\\nels like AnomalyCLIP, AnomalyGPT,GPU availability\\nis essential for efficient training and inference. We en-\\ncountered several infrastructure issues that highlighted\\nthe importance of robust hardware setups and efficient\\nresource management.\\n• Parameter Tuning: The CLIP model,while powerful,\\nrequires careful parameter tuning to optimize perfor-\\nmance. This manual parameter setting process can be\\ntime-consuming and prone to errors,potentially lead-\\ning to overfitting if not managed properly. This chal-\\nlenge underscored the need for automated parameter\\noptimization techniques to streamline model training.\\nAnomalyGPT and AnomalyCLIP work on huge num-\\nber of parameters and in our case it was 7B.Not only\\nit will be tricky to manually update the parameters but\\nalso time consuming. The models used for these were\\npretrained on these parameters and weights generated\\nwere huge files.\\n• Model Overfitting: The tendency of the CLIP model\\nto overfit on the training dataset was a notable draw-\\nback. This overfitting occurs when the model learns\\nthe training data too well,including its noise and out-\\nliers,at the expense of generalizing to new,unseen data.\\nIdentifying and mitigating overfitting through tech-\\nniques such as cross-validation and regularization is\\ncrucial for developing robust models. AnomalyGPT\\nand AnomalyCLIP are not prone to overfitting since\\nthey work on huge number of parameters but can take\\na lot of time to fine tune them before reaching the de-\\nsired behaviour.\\n• Diverse Test Data : The performance of our models\\ncan vary significantly with more diverse and complex\\ntest images. While the models performed well on the\\ngiven dataset,their accuracy and reliability may de-\\ncrease when exposed to a broader range of anomalies.\\nThis observation emphasizes the importance of com-\\nprehensive testing on diverse datasets to ensure model\\ngeneralization and robustness.\\n9.2. Enhancements and Future Work\\n• Infrastructure Improvements: Investing in more re-\\nliable and scalable GPU infrastructure would signif-\\nicantly enhance the efficiency and effectiveness of\\nmodel training and deployment. Exploring cloud-\\nbased solutions could provide the necessary computa-\\ntional power while maintaining flexibility and scalabil-\\nity.\\n• Automated Parameter Optimization: Implementing\\nautomated parameter tuning methods, such as grid\\nsearch,random search,or more advanced techniques\\nlike Bayesian optimization,can reduce the manual ef-\\nfort required and help find optimal settings more effi-\\nciently. This automation would also help mitigate the\\nrisk of overfitting.\\n• Regularization Techniques : To address overfit-\\nting, incorporating regularization techniques such as\\ndropout,weight decay, and data augmentation can im-\\nprove model generalization. Additionally, implement-\\ning early stopping based on validation performance\\ncan prevent the model from overfitting during training.\\n12'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 12}, page_content='• Expanded Dataset : To enhance the model’s ability\\nto handle diverse and complex anomalies,expanding\\nthe dataset to include a wider variety of images would\\nbe beneficial. This expansion could involve collect-\\ning more images with different types of anomalies and\\nnon-anomalies across various contexts.\\n• Continuous Evaluation and Feedback Loop : Es-\\ntablishing a continuous evaluation and feedback loop\\nwhere the model is regularly tested on new data and\\nfine-tuned based on performance metrics can ensure\\nongoing improvement and adaptation to new types of\\nanomalies.\\n• Integration with User Feedback : For the Anoma-\\nlyGPT model, integrating user feedback mechanisms\\nwhere users can provide corrections or confirmations\\nof the model’s predictions can help in continuously re-\\nfining and improving the model’s accuracy and relia-\\nbility.\\nBy addressing these areas,we can enhance the robust-\\nness, accuracy,and generalizability of our anomaly detec-\\ntion models,making them more effective tools for real-\\nworld applications.\\n10. Conclusion\\nWe worked on 3 different versions of anomaly detec-\\ntion models with different settings and configurations and\\nalgorithms. Each worked differently and provided differ-\\nent classification results depending on the hyperparameter\\ntuning done for each. With 3 different models we have\\ntried to compare the behaviour,accuracy.Each model was\\nadding a new capability,with CLIP being a regular vision\\nmodel which just classified the paintings versus,anomaly\\nCLIP highlighting anomalies in those as well and in the\\nend,anomalyGPT providing a chatbot approach to not only\\ndetect anomalies but also describe the image and interact\\nwith the user. We also feel that it might largely depend\\non the dataset as well which in turn decides which algo-\\nrithm/model approach would eventually be best for that.\\nHere in our case,we had different variations of anoma-\\nlies tested and each model provided different output based\\non how they work. It also depends what is the need of\\nconsumer which ultimately decides what model would be\\nbest for that scenario,keeping its requirements/challenges in\\nmind as discussed in report.\\nReferences\\n[1] Z. Gu, B. Zhu, G. Zhu, Y . Chen, M. Tang,\\nand J. Wang, “Anomalygpt: Detecting industrial\\nanomalies using large vision-language models,” 2023.\\n[Online]. Available: https://arxiv.org/abs/2308.15366\\n1, 2, 3\\n[2] M. Authors, “Llama weights: An ultimate guide\\n2024,” 2024, llama Weights: An Ultimate Guide 2024.\\n2, 3\\n[3] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny,\\n“Minigpt-4: Enhancing vision-language understand-\\ning with advanced large language models,” 2023.\\n[Online]. Available: https://arxiv.org/abs/2304.10592\\n3\\n[4] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-\\n2: Bootstrapping language-image pre-training with\\nfrozen image encoders and large language models,”\\n2023. [Online]. Available: https://arxiv.org/abs/2301.\\n12597 3\\n[5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu,\\nH. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\\nGonzalez, I. Stoica, and E. P. Xing, “Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%*\\nchatgpt quality,” March 2023. [Online]. Available:\\nhttps://lmsys.org/blog/2023-03-30-vicuna/ 3\\n[6] Y . Su, T. Lan, H. Li, J. Xu, Y . Wang, and\\nD. Cai, “Pandagpt: One model to instruction-\\nfollow them all,” 2023. [Online]. Available: https:\\n//arxiv.org/abs/2305.16355 3\\n[7] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V .\\nAlwala, A. Joulin, and I. Misra, “Imagebind: One\\nembedding space to bind them all,” 2023. [Online].\\nAvailable: https://arxiv.org/abs/2305.05665 3, 5\\n[8] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh,\\nG. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\\nJ. Clark, G. Krueger, and I. Sutskever, “Learning\\ntransferable visual models from natural language\\nsupervision,” 2021. [Online]. Available: https:\\n//arxiv.org/abs/2103.00020 4\\n[9] J. Boschman, “Clip paper explained\\neasily in 3 levels of detail,” https://\\nmedium.com/one-minute-machine-learning/\\nclip-paper-explained-easily-in-3-levels-of-detail-61959814ad13,\\n2023, cLIP Summary. 4\\n[10] P. Potrimba, “What is resnet-50?” https://blog.\\nroboflow.com/what-is-resnet-50/, 2024, resNet50. 4\\n13'),\n",
       " Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 13}, page_content='[11] P. Subedi, “Clip by openai explained,”\\nhttps://medium.com/@pragyansubedi/\\nclip-by-openai-explained-1e4c38644356, 2023,\\ncrossentropy. 4\\n[12] A. 416, “Anomaly detection using iso-\\nlation forest – a complete guide,” https:\\n//www.analyticsvidhya.com/blog/2021/07/\\nanomaly-detection-using-isolation-forest-a-complete-guide,\\n2024, iso forest. 4\\n[13] P. kumar, “Understanding lof (local out-\\nlier factor) —perspective for implemen-\\ntation,” https://medium.com/@pramodch/\\nunderstanding-lof-local-outlier-factor-for-implementation-1f6d4ff13ab9,\\n2020, lOF. 4\\n[14] V . Jayaswal, “Local outlier factor (lof)\\n— algorithm for outlier identifica-\\ntion,” https://towardsdatascience.com/\\nlocal-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843,\\n2020, lOF Maths. 4\\n[15] Q. Zhou, G. Pang, Y . Tian, S. He, and J. Chen,\\n“Anomalyclip: Object-agnostic prompt learning\\nfor zero-shot anomaly detection,” 2024. [Online].\\nAvailable: https://arxiv.org/abs/2310.18961 4, 7\\n[16] A. Authors, “Anomalygtp github code repository,”\\nhttps://github.com/CASIA-IV A-Lab/AnomalyGPT.\\ngit, 2023, anomalygtp Github Code Repository. 5\\n[17] E. Betzalel, “image outlier detection,” https:\\n//github.com/eyalbetzalel/image outlier detection.git,\\n2022, code for CLIP model. 7\\n[18] J. Wook, “Clip,” https://github.com/openai/CLIP.git,\\n2021, code for CLIP model. 7\\n[19] ICARO, “Best artworks of all time,”\\nhttps://www.kaggle.com/datasets/ikarus777/\\nbest-artworks-of-all-time, 2019, accessed: 2024-\\n05-23.\\n14')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a374eb7c-e262-42bb-8f3f-308ba7dcdbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29633e3b-ff24-4ace-a09b-c03b6e28c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  76\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100b7d1a-1209-49d4-99ed-c51bc233a938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'AnomalyGPT_Practicum.pdf', 'page': 0}, page_content='Sandia Laboratories Project-AnomalyGPT\\nTeam SAM\\nSeungHoon Yoo\\nGeorgia Institute of Technology\\nsyoo97@gatech.edu@gatech.edu\\nAmit Trikha\\nGeorgia Institute of Technology\\namittrikha@gatech.edu\\nMukta Bisht\\nGeorgia Institute of Technology\\nmbisht6@gatech.edu\\nAbstract\\nFor this study, we utilized the AnomalyGPT model [1]\\nto specialize in detecting anomalies in our custom Art-\\nwork (paintings) dataset. Large vision-language models\\n(LVLM) excel in recognizing common objects due to their\\nextensive training data, they often struggle with domain-\\nspecific knowledge and fine-grained details within objects.\\nThis limitation impedes their effectiveness in domain spe-\\ncific tasks such as Artwork (or painting) Anomaly De-\\ntection. So,we investigated adapting the AnomalyGPT\\nmodel to our custom dataset to tackle the domain spe-\\ncific (i.e artwork/painting) problem. Our approach uti-\\nlizing the AnomalyGPT models,introduces a methodology\\nleveraging Large Vision-Language Models (LVLMs) to en-')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c5113",
   "metadata": {},
   "source": [
    "# Get an API key: \n",
    "\n",
    " https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file\n",
    "\n",
    " Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1073ab7f-2632-4367-8dec-c19449d6ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragapp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05168594419956207,\n",
       " -0.030764883384108543,\n",
       " -0.03062233328819275,\n",
       " -0.02802734263241291,\n",
       " 0.01813093200325966]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]\n",
    "#vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688b6e6a-d8ab-41fb-a665-b72c9c9b4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c674c5c-1b57-42e9-a99d-9e882c75da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is new in Development of Multiple Combined Regression Methods for Rainfall Measurement paper?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c5c6bb-fd0e-45ec-b315-e3f7656e0329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf21d394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f5d777dd-deb0-4200-9d07-840db3155380', metadata={'page': 3, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='putational complexity and large datasets required for these\\nmodels,while the standard CLIP model could be run lo-\\ncally without such requirements. ¡FOR EACH MODEL\\nADD FEW SENTENCES WHICH W AS DONE SPECIFI-\\n4'),\n",
       " Document(id='73b7d3a3-4470-4785-a053-9f2e02a2d4b9', metadata={'page': 10, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='images. The team explored various options and eventually\\naddressed the issue by combining multiple medical datasets\\nfrom different sources.\\n8. Comparison between models\\nThe table 5 offers a detailed comparison of accu-\\nracy scores for different models tested on a dataset,\\n11'),\n",
       " Document(id='eabd1bb3-b6fa-4bb0-b970-0997d7f5e52b', metadata={'page': 11, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='plexity of the models and the challenges in training them\\non datasets. Capturing intricate details proves to be particu-\\nlarly difficult. This complexity arises from the sophisticated\\nnature of the models, requiring extensive computational re-\\nsources and fine-tuning to achieve accurate results. The\\ntraining process involves navigating various parameters and\\nconfigurations to optimize performance,making it a daunt-\\ning task. Additionally,the models must be capable of rec-\\nognizing subtle patterns and nuances within the data,which\\nadds another layer of difficulty.\\nModel Algorithm Accuracy score\\nCLIP resnet50 100\\ncrossentropy 83.67\\niso forest 91.11\\nLOF 97.62\\nAnomalyCLIP - 71.11\\nAnomalyGPT - 59.7\\nTable 5. Accuracy Score Comparison for Different Models on Test\\nDataset\\n9. Learning and Future enhancements\\n9.1. Key Learnings\\n• Model Performance and Infrastructure: One of the\\nmost significant learnings from this project was the\\ncritical role of infrastructure in model training and de-'),\n",
       " Document(id='ebe98c35-f4f8-4494-808b-40543be59031', metadata={'page': 4, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='CALLY FOR THAT MODEL DURING TRAINING AND\\nSETUP¿\\n5.1. AnomalyGPT\\nOur implementation for AnomalyGPT begins with repli-\\ncating the methodology outlined in the original Anoma-\\nlyGPT research paper, which focuses on utilizing indus-\\ntrial images for effective anomaly detection and then sub-\\nsequently training the model on a new dataset. The initial\\nstep involved securing access to the model weights and in-\\ntegrating them into both the training and inference phases\\nof our project. This process ensures that we leverage the\\nestablished capabilities of AnomalyGPT in our specific do-\\nmain of interest that is industrial dataset. Later, we sourced\\na new Artworks dataset, expanding beyond the industrial\\ncontext studied in the original paper. Artwork dataset is then\\naugmented with anomalies, introduced manually or through\\nautomated procedures (python notebooks), to simulate real-\\nworld scenarios where anomalies may manifest in the im-\\nagery. The goal was to train the AnomalyGPT model using'),\n",
       " Document(id='2dc232ce-f01b-4202-9609-9b073f8bcc54', metadata={'page': 2, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='are the adjustable parameters that the model learns during\\ntraining,enabling it to capture patterns in the data and per-\\nform well on natural language processing (NLP) tasks. The\\ntransformer architecture 4,which has become the prevalent\\ndesign for state-of-the-art language models,organizes these\\nweights into a specific structure named ”multi-head self-\\nattention”. MiniGPT4 [3] does this by connecting BLIP-2’s\\n[4] image part with the Vicuna [5] model through a special\\nlayer,refining its performance with lots of image and text\\ndata. Similarly,PandaGPT [6] links ImageBind [7] with the\\nVicuna [5] model to handle different kinds of information\\ntogether. While these models show promise in handling var-\\nious types of tasks,they often lack specific knowledge about\\nparticular fields because they are trained on broad datasets\\nthat cover many topics. To tackle this issue,AnomalyGPT\\npresents a new approach in this research. It uses simulated'),\n",
       " Document(id='16c124ca-8361-4df7-8cf3-4ef69876718d', metadata={'page': 11, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='AnomalyGPT and AnomalyCLIP work on huge num-\\nber of parameters and in our case it was 7B.Not only\\nit will be tricky to manually update the parameters but\\nalso time consuming. The models used for these were\\npretrained on these parameters and weights generated\\nwere huge files.\\n• Model Overfitting: The tendency of the CLIP model\\nto overfit on the training dataset was a notable draw-\\nback. This overfitting occurs when the model learns\\nthe training data too well,including its noise and out-\\nliers,at the expense of generalizing to new,unseen data.\\nIdentifying and mitigating overfitting through tech-\\nniques such as cross-validation and regularization is\\ncrucial for developing robust models. AnomalyGPT\\nand AnomalyCLIP are not prone to overfitting since\\nthey work on huge number of parameters but can take\\na lot of time to fine tune them before reaching the de-\\nsired behaviour.\\n• Diverse Test Data : The performance of our models\\ncan vary significantly with more diverse and complex'),\n",
       " Document(id='f7f105fa-5552-445d-909a-e6c3c0065ec8', metadata={'page': 11, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='a lot of time to fine tune them before reaching the de-\\nsired behaviour.\\n• Diverse Test Data : The performance of our models\\ncan vary significantly with more diverse and complex\\ntest images. While the models performed well on the\\ngiven dataset,their accuracy and reliability may de-\\ncrease when exposed to a broader range of anomalies.\\nThis observation emphasizes the importance of com-\\nprehensive testing on diverse datasets to ensure model\\ngeneralization and robustness.\\n9.2. Enhancements and Future Work\\n• Infrastructure Improvements: Investing in more re-\\nliable and scalable GPU infrastructure would signif-\\nicantly enhance the efficiency and effectiveness of\\nmodel training and deployment. Exploring cloud-\\nbased solutions could provide the necessary computa-\\ntional power while maintaining flexibility and scalabil-\\nity.\\n• Automated Parameter Optimization: Implementing\\nautomated parameter tuning methods, such as grid\\nsearch,random search,or more advanced techniques'),\n",
       " Document(id='ecfd172b-903b-404b-8f51-1f833ddb4e37', metadata={'page': 11, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='9.1. Key Learnings\\n• Model Performance and Infrastructure: One of the\\nmost significant learnings from this project was the\\ncritical role of infrastructure in model training and de-\\nployment. Given the computational demands of mod-\\nels like AnomalyCLIP, AnomalyGPT,GPU availability\\nis essential for efficient training and inference. We en-\\ncountered several infrastructure issues that highlighted\\nthe importance of robust hardware setups and efficient\\nresource management.\\n• Parameter Tuning: The CLIP model,while powerful,\\nrequires careful parameter tuning to optimize perfor-\\nmance. This manual parameter setting process can be\\ntime-consuming and prone to errors,potentially lead-\\ning to overfitting if not managed properly. This chal-\\nlenge underscored the need for automated parameter\\noptimization techniques to streamline model training.\\nAnomalyGPT and AnomalyCLIP work on huge num-\\nber of parameters and in our case it was 7B.Not only\\nit will be tricky to manually update the parameters but'),\n",
       " Document(id='9db32072-8064-4235-90f2-1e4b7c4b2db4', metadata={'page': 6, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='and pearl earring.py, respectively. Similarly,the original\\ntest.sh file was modified to create monalisa test.sh and\\npearl earring test.sh shell files.\\n6. Model Testing\\nAll the three models were tested using same test images\\nwhich comprised 34 anomaly and 16 non anomaly images.\\nBelow is the brief summary of testing for each model.\\n7'),\n",
       " Document(id='326befba-0899-4300-bd9b-95ddd74f0689', metadata={'page': 11, 'source': 'AnomalyGPT_Practicum.pdf'}, page_content='ity.\\n• Automated Parameter Optimization: Implementing\\nautomated parameter tuning methods, such as grid\\nsearch,random search,or more advanced techniques\\nlike Bayesian optimization,can reduce the manual ef-\\nfort required and help find optimal settings more effi-\\nciently. This automation would also help mitigate the\\nrisk of overfitting.\\n• Regularization Techniques : To address overfit-\\nting, incorporating regularization techniques such as\\ndropout,weight decay, and data augmentation can im-\\nprove model generalization. Additionally, implement-\\ning early stopping based on validation performance\\ncan prevent the model from overfitting during training.\\n12')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1c8321-1efd-4a11-9744-0d1a7c6f4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnomalyGPT and AnomalyCLIP work on huge num-\n",
      "ber of parameters and in our case it was 7B.Not only\n",
      "it will be tricky to manually update the parameters but\n",
      "also time consuming. The models used for these were\n",
      "pretrained on these parameters and weights generated\n",
      "were huge files.\n",
      "• Model Overfitting: The tendency of the CLIP model\n",
      "to overfit on the training dataset was a notable draw-\n",
      "back. This overfitting occurs when the model learns\n",
      "the training data too well,including its noise and out-\n",
      "liers,at the expense of generalizing to new,unseen data.\n",
      "Identifying and mitigating overfitting through tech-\n",
      "niques such as cross-validation and regularization is\n",
      "crucial for developing robust models. AnomalyGPT\n",
      "and AnomalyCLIP are not prone to overfitting since\n",
      "they work on huge number of parameters but can take\n",
      "a lot of time to fine tune them before reaching the de-\n",
      "sired behaviour.\n",
      "• Diverse Test Data : The performance of our models\n",
      "can vary significantly with more diverse and complex\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f991a1f-6ce9-4463-9941-b35014df94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee17439-7bc3-4931-9f57-4ec7e82ce902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "266e86e0-746b-4943-9470-fd842633ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9db9500d-4c51-4a10-9b21-f1ef9c8f985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper discusses training AnomalyGPT, a model for anomaly detection, on an \"Artwork\" dataset of images of the Mona Lisa and Girl with a Pearl Earring.  The training process involved various image transformations and hyperparameter adjustments (learning rate of 1e-3, batch sizes of 8 and 16). The paper also briefly mentions testing with CLIP and includes comparisons with other models like MiniGPT4 and PandaGPT.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is in this paper?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff65d0-2436-47f8-8572-6979a3378701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
